{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1LUKOSfYpVj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts=[\"I love python programming\",\n",
        "        \"RNNs are powerful for sequence data\",\n",
        "        \"tokenization is the first step\"]"
      ],
      "metadata": {
        "id": "Gfk8mZIWZAng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()"
      ],
      "metadata": {
        "id": "EkTUeSmRZgJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(texts)"
      ],
      "metadata": {
        "id": "4sEc72h1ZggU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences=tokenizer.texts_to_sequences(texts)"
      ],
      "metadata": {
        "id": "DwmeNO2UZg4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences=pad_sequences(sequences,padding=\"post\")"
      ],
      "metadata": {
        "id": "hdM6QX7QZhLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"word index:\\n\",tokenizer.word_index)\n",
        "print(\"sequences:\\n\",sequences)\n",
        "print(\"padded sequences:\\n\",padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiJjudCsa55X",
        "outputId": "b38f8536-8057-49fe-c002-f5388b3db460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word index:\n",
            " {'i': 1, 'love': 2, 'python': 3, 'programming': 4, 'rnns': 5, 'are': 6, 'powerful': 7, 'for': 8, 'sequence': 9, 'data': 10, 'tokenization': 11, 'is': 12, 'the': 13, 'first': 14, 'step': 15}\n",
            "sequences:\n",
            " [[1, 2, 3, 4], [5, 6, 7, 8, 9, 10], [11, 12, 13, 14, 15]]\n",
            "padded sequences:\n",
            " [[ 1  2  3  4  0  0]\n",
            " [ 5  6  7  8  9 10]\n",
            " [11 12 13 14 15  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"i love python programming\"\n",
        "char_tokens=list(text)\n",
        "print(\"character tokens:\",char_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXqIfHZ1IJWO",
        "outputId": "27ea4948-dc4a-4e84-aab3-b4112c744ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "character tokens: ['i', ' ', 'l', 'o', 'v', 'e', ' ', 'p', 'y', 't', 'h', 'o', 'n', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "char_tokenized = [list(text) for text in texts]\n",
        "for i, tokens in enumerate(char_tokenized):\n",
        "    print(f\"\\nText {i+1}:\")\n",
        "    print(\"Original:\", texts[i])\n",
        "    print(\"Character Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkAGwC2tJrNB",
        "outputId": "349c25d7-9e1d-4009-8c76-733fdd139cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text 1:\n",
            "Original: I love python programming\n",
            "Character Tokens: ['I', ' ', 'l', 'o', 'v', 'e', ' ', 'p', 'y', 't', 'h', 'o', 'n', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g']\n",
            "\n",
            "Text 2:\n",
            "Original: RNNs are powerful for sequence data\n",
            "Character Tokens: ['R', 'N', 'N', 's', ' ', 'a', 'r', 'e', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'f', 'o', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'd', 'a', 't', 'a']\n",
            "\n",
            "Text 3:\n",
            "Original: tokenization is the first step\n",
            "Character Tokens: ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'f', 'i', 'r', 's', 't', ' ', 's', 't', 'e', 'p']\n"
          ]
        }
      ]
    }
  ]
}